################################################################################
# Name:		create_pcluster.yml
# Author:	Rodney Marable <rodney.marable@gmail.com>
# Created On:	April 20, 2019
# Last Changed:	June 27, 2019
# Purpose:	Ansible playbook to create new ParallelCluster stacks
# Notes:        EFS and FSxL are handled by Parallelcluster
################################################################################

---

- name: Provision a new ParallelCluster stack
  hosts: local
  connection: local
  gather_facts: false
  vars_files:
    - vars_files/{{ cluster_name }}.yml
  vars:
    - local_homedir: "{{ lookup('env','HOME') }}"
    - local_workingdir: "{{ lookup('pipe','pwd') }}"
    - local_os: "{{ lookup('pipe','uname') }}"

  tasks:
    - name: Start a timer for the cluster environment build process
      command: date +%Y-%m-%d\ \@\ %H:%M:%S
      register: start_overall_timer

    - debug: 
        msg: "Debug mode => {{ debug_mode }}"

    - name: Create a local state directory for this cluster
      file:
        path: "{{ cluster_data_dir }}"
        state: directory
        mode: 0755

    - name: Create an SNS topic to send notifications to cluster_owner_email
      sns_topic:
        name: "sns_alerts_{{ cluster_name }}"
        region: "{{ region }}"
        state: present
        display_name: "SNS Alerts for Cluster {{ cluster_name }}"
        subscriptions:
          - endpoint: "{{ cluster_owner_email }}"
            protocol: "email"

    - name: Send an SNS notification announcing the cluster build initiation
      sns:
        msg: "Started building {{ cluster_name }} at {{ start_overall_timer.stdout }}"
        message_structure: json
        subject: "Cluster Deployment Update: {{ cluster_name }}"
        topic: "sns_alerts_{{ cluster_name }}"
        region: "{{ region }}"
      delegate_to: localhost

    - name: Create s3_bucketname to support this cluster stack
      s3_bucket:
        name: "{{ s3_bucketname }}"
        tags:
          ClusterID: "{{ cluster_name }}"
          ClusterStackType: ParallelCluster
          ClusterOSType: "{{ base_os }}"
          ClusterScheduler: "{{ scheduler }}"
          ClusterSerialNumber: "{{ cluster_serial_number }}"
          ClusterOwner: "{{ cluster_owner }}"
          ClusterOwnerEmail: "{{ cluster_owner_email }}"
          ClusterOwnerDepartment: "{{ cluster_owner_department }}"
          ProdLevel: "{{ prod_level }}"
          DEPLOYMENT_DATE: "{{ DEPLOYMENT_DATE }}"
      when: '"UNDEFINED" in project_id'

    - name: Create s3_bucketname to support this cluster stack and append the ProjectID tag 
      s3_bucket:
        name: "{{ s3_bucketname }}"
        tags:
          ClusterID: "{{ cluster_name }}"
          ClusterStackType: ParallelCluster
          ClusterOSType: "{{ base_os }}"
          ClusterScheduler: "{{ scheduler }}"
          ClusterSerialNumber: "{{ cluster_serial_number }}"
          ClusterOwner: "{{ cluster_owner }}"
          ClusterOwnerEmail: "{{ cluster_owner_email }}"
          ClusterOwnerDepartment: "{{ cluster_owner_department }}"
          ProjectID: "{{ project_id }}"
          ProdLevel: "{{ prod_level }}"
          DEPLOYMENT_DATE: "{{ DEPLOYMENT_DATE }}"
      when: '"UNDEFINED" not in project_id'
          
    - name: Create a new security group for mounting external NFS file systems
      ec2_group:
        name: pcluster-{{ cluster_name }}-externalNfs
        description: Permit NFS traffic to/from external NFS file systems
        vpc_id: "{{ vpc_id }}"
        region: "{{ region }}"
        rules:
          - proto: tcp
            ports:
              - 111
              - 2049
              - 4045
              - 4046
              - 4047
          - proto: udp
            ports:
              - 111
              - 2049
              - 4045
              - 4046
              - 4047
            cidr_ip: 172.31.0.0/16
      register: external_nfs_sg
      when: 'enable_external_nfs == "true"'

    - name: Generate a new EC2 keypair for this cluster
      ec2_key:
        name: "{{ ec2_keypair }}"
        region: "{{ region }}"
      no_log: true
      register: ec2_private_key

    - name: Save the private key
      copy:
        content: "{{ ec2_private_key.key.private_key }}"
        dest: "{{ ssh_keypair }}"
        mode: 0600
      when: ec2_private_key.changed

    - name: Create local staging directories for cluster data and file transfers
      file:
        path: "{{ item }}"
        state: directory
        mode: 0755
      with_items:
        - "{{ stage_dir }}"
        - "{{ performance_stage_dir }}"
        - "{{ serverless_stage_dir }}"
        - "{{ serverless_template_dir }}"

    - name: Template custom scripts (preinstall, postinstall, and generate_cron_lifetime_string) and the cluster config file
      template:
        src: "{{ item.src }}"
        dest: "{{ item.dest }}"
        mode: 0755
      with_items:
        - { src: '{{ cluster_config_template_orig }}', dest: '{{ cluster_config_template }}' }
        - { src: '{{ preinstall_template_orig }}', dest: '{{ preinstall_src }}' }
        - { src: '{{ postinstall_template_orig }}', dest: '{{ postinstall_src }}' }
        - { src: '{{ generate_cron_lifetime_string_src }}', dest: '{{ generate_cron_lifetime_string_dest }}' }

    - name: PUT the preinstall script, postinstall script, and cluster config into s3_bucketname
      aws_s3:
        bucket: "{{ s3_bucketname }}"
        src: "{{ item.src }}"
        object: "{{ item.dest }}"
        permission: public-read
        encrypt: False
        mode: put
      with_items:
        - { src: '{{ cluster_config_template }}', dest: '{{ s3_script_path }}/{{ cluster_config_dest }}' }
        - { src: '{{ preinstall_src }}', dest: '{{ s3_script_path }}/{{ preinstall_s3_dest }}' }
        - { src: '{{ postinstall_src }}', dest: '{{ s3_script_path }}/{{ postinstall_s3_dest }}' }

    - name: Template the cluster SSH access and kill-pcluster shell scripts to stage_dir
      template:
        src: "{{ item.src }}"
        dest: "{{ item.dest }}"
        mode: 0755
      with_items:
        - { src: '{{ cluster_template_dir }}/access_cluster.j2', dest: '{{ stage_dir }}/access_cluster.{{ cluster_name }}.py' }
        - { src: '{{ cluster_template_dir }}/kill_pcluster.j2', dest: '{{ stage_dir }}/kill_pcluster.{{ cluster_name }}.sh' }

    - block:
      - name: Template the external NFS file system mount list to the cluster state directory
        template:
          src: "{{ external_nfs_mount_list_template_orig }}"
          dest: "{{ external_nfs_mount_list_template_src }}"
          mode: 0755
      - name: Template the external NFS file system mount list to the staging directory
        template:
          src: "{{ external_nfs_mount_list_template_orig }}"
          dest: "{{ stage_dir }}/{{ external_nfs_mount_list_template_dest }}"
          mode: 0755
      - name: PUT the external NFS file system mount list into s3_bucketname
        aws_s3:
          bucket: "{{ s3_bucketname }}"
          src: "{{ external_nfs_mount_list_template_src }}"
          object: "{{ s3_script_path }}/{{ external_nfs_mount_list_template_dest }}"
          permission: public-read
          encrypt: False
          mode: put
      when: 'enable_external_nfs == "true"'

    - block:
      - name: Template the common HPC performance shell scripts to stage_dir
        template:
          src: "{{ performance_template_dir }}/{{ item }}.j2"
          dest: "{{ performance_stage_dir }}/{{ item }}.{{ cluster_name }}.sh"
          mode: 0755
        with_items:
          - bang
          - combine_csv_summary_files_for_plotting
          - perf-standalone-test
      - name: Template the custom SGE-specific HPC performance shell scripts to stage_dir
        template:
          src: "{{ performance_template_dir }}/{{ item }}.j2"
          dest: "{{ performance_stage_dir }}/{{ item }}.{{ cluster_name }}.sh"
          mode: 0755
        with_items:
          - combine_sge_data_files_for_plotting
          - perf-qsub
          - create_sge_task_array_csv_files
        when: 'scheduler == "sge"'
      - name: Template the custom Slurm-specific HPC performance shell scripts to stage_dir
        template:
          src: "{{ performance_template_dir }}/{{ item }}.j2"
          dest: "{{ performance_stage_dir }}/{{ item }}.{{ cluster_name }}.sh"
          mode: 0755
        with_items:
          - perf-sbatch
        when: 'scheduler == "slurm"'
      - name: Generate custom performance test qsub scripts for SGE
        shell: sh {{ performance_rootdir }}/{{ item }}.sh {{ perftest_custom_start_number }} {{ perftest_custom_step_size }} {{ perftest_custom_total_tests }}
        args:
          chdir: "{{ performance_rootdir }}"
        with_items:
          -  generate_qsub_custom_templates
        when: 'scheduler == "sge"'
      - name: Generate custom performance test sbatch scripts for Slurm
        shell: sh {{ performance_rootdir }}/{{ item }}.sh {{ perftest_custom_start_number }} {{ perftest_custom_step_size }} {{ perftest_custom_total_tests }}
        args:
          chdir: "{{ performance_rootdir }}"
        with_items:
          -  generate_sbatch_custom_templates
        when: "scheduler == 'slurm'"
      - name: Template the custom performance test qsub scripts for SGE to stage_dir
        template:
          src: "{{ item }}"
          dest: "{{ performance_stage_dir }}/{{ item | basename | regex_replace('.j2','') }}.{{ cluster_name }}.sh"
          mode: 0755
        with_fileglob:
          - "{{ performance_template_dir }}/qsub-*.j2"
        when: "scheduler == 'sge'"
      - name: Template the custom performance test sbatch scripts for Slurm to stage_dir
        template:
          src: "{{ item }}"
          dest: "{{ performance_stage_dir }}/{{ item | basename | regex_replace('.j2','') }}.{{ cluster_name }}.sh"
          mode: 0755
        with_fileglob:
          - "{{ performance_template_dir }}/sbatch-*.j2"
        when: "scheduler == 'slurm'"
      - name: Template the custom performance Python scripts to stage_dir
        template:
          src: "{{ performance_template_dir }}/{{ item }}.j2"
          dest: "{{ performance_stage_dir }}/{{ item }}.{{ cluster_name }}.py"
          mode: 0755
        with_items:
          - hashtest
          - fibonacci_hashtest
          - print_fibonacci
      - name: Copy the Axb_random matrix test configuration file to stage_dir
        command: cp {{ performance_rootdir }}/MATRIX_SIZES.conf {{ performance_stage_dir }}/MATRIX_SIZES.conf
      - name: Copy the common performance shell and Python scripts to stage_dir
        command: cp {{ performance_rootdir }}/{{ item }} {{ performance_stage_dir }}
        with_items:
          - Axb_random.py
          - compress_logfiles.py
          - bite_Axb_random.sh
          - bite_fibonacci_hashtest.sh
          - bite_hashtest.sh
          - cleanup_performance.sh
          - csv_summary_time_measurement.sh
      - name: Copy the common performance shell and Python scripts to stage_dir
        command: cp {{ performance_rootdir }}/{{ item }} {{ performance_stage_dir }}
        with_items:
          - make_sge_cluster_plots.py
          - rebuild_sge_csv.sh
        when: "scheduler == 'sge'"
      - name: Copy the ParallelClusterMaker performance toolkit documentation to stage_dir
        command: cp {{ item }} {{ performance_stage_dir }}
        with_fileglob:
          - "{{ performance_rootdir }}/README*.*"
      when:
        - enable_hpc_performance_tests == "true"
        - scheduler != "awsbatch"

    - name: Start the stack creation timer
      command: date +%Y-%m-%d\ \@\ %H:%M:%S
      register: start_stack_creation_timer

    - debug:
        msg:
          - ""
          - "=================================================================="
          - "                   Cluster Launch Summary Report"
          - "=================================================================="
          - ""
          - "Cluster StackName: {{ cluster_name }}"
          - "SerialDateStamp:   {{ cluster_serial_datestamp }}"
          - "Launch Timestamp:  {{ start_stack_creation_timer.stdout }}"
          - "Operating System:  {{ base_os }}"
          - "HPC Scheduler:     {{ scheduler }}" 
          - "Availabilty Zone:  {{ az }}"
          - "EFA Enabled:       {{ enable_efa | bool | upper }}"
          - "EFS Enabled:       {{ enable_efs | bool | upper }}"
          - "FSxL Enabled:      {{ enable_fsx | bool | upper }}"
          - "External NFS:      {{ enable_external_nfs | bool | upper }}"
          - ""
          - "This process will take about 45 minutes to complete."
          - ""
          - "Run this command to monitor the stack progress in real time:"
          - ""
          - "pcluster status --region {{ region }} {{ cluster_name }}"
          - ""
      when: 'scheduler != "awsbatch"'

    - debug:
        msg:
          - ""
          - "=================================================================="
          - "                   Cluster Launch Summary Report"
          - "=================================================================="
          - ""
          - "Cluster StackName: {{ cluster_name }}"
          - "SerialDateStamp:   {{ cluster_serial_datestamp }}"
          - "Launch Timestamp:  {{ start_stack_creation_timer.stdout }}"
          - "Operating System:  {{ base_os }}"
          - "HPC Scheduler:     {{ scheduler }}" 
          - "Availabilty Zone:  {{ az }}"
          - "EFA Enabled:       {{ enable_efa | bool | upper }}"
          - "EFS Enabled:       {{ enable_efs | bool | upper }}"
          - "External NFS:      {{ enable_external_nfs | bool | upper }}"
          - ""
          - "This process will take about 20 minutes to complete."
          - ""
          - "Run this command to monitor the stack progress in real time:"
          - ""
          - "pcluster status --region {{ region }} {{ cluster_name }}"
          - ""
      when: 'scheduler == "awsbatch"'

    - name: Launch the new ParallelCluster stack
      command: pcluster create --config {{ cluster_config_template }} --region {{ region }} --norollback {{ cluster_name }}

    - block:
      - name: Parse the master instance security group name
        shell: >
            aws --region {{ region }} ec2 describe-security-groups --filters "Name=tag:aws:cloudformation:logical-id,Values=MasterSecurityGroup" "Name=tag:ClusterSerialNumber,Values={{ cluster_serial_number }}" | jq '.SecurityGroups[].GroupName' | tr -d \"
        register: master_instance_sg_name
      - name: Permit web traffic to the master instance (potentially insecure!)
        ec2_group:
          name: "{{ master_instance_sg_name.stdout }}"
          description: Enable access to the Master host
          vpc_id: "{{ vpc_id }}"
          region: "{{ region }}"
          purge_rules: false
          rules:
            - proto: tcp
              ports:
                - 80
                - 443
              cidr_ip: 0.0.0.0/0
      when: 'enable_ganglia == "true"'

    - name: Stop the stack timer
      command: date +%Y-%m-%d\ \@\ %H:%M:%S
      register: stop_stack_creation_timer

    - name: Set cluster_start_time
      command: date +%Y-%m-%d\ %H:%M:%S
      register: cluster_start_time

    - name: Start lambda_timer
      command: date +%Y-%m-%d\ \@\ %H:%M:%S
      register: start_lambda_timer

    - name: Generate a schedule for execution of the Lambda cluster stack termination function
      shell: ./generate_cron_lifetime_string.{{ cluster_name }}.py --cluster_lifetime="{{ cluster_lifetime }}" --cluster_serial_number_file="{{ cluster_serial_number_file }}"
      args:
        chdir: "{{ cluster_data_dir }}"
      register: cron_lifetime_string_raw

    - name: Parse the result of the Lambda function schedule generator
      set_fact:
        cron_lifetime_string: "{{ cron_lifetime_string_raw.stdout }}"

    - name: Template the terminate_pcluster Lambda function to the serverless staging directory
      template:
        src: "{{ serverless_template_dir }}/{{ item.src }}"
        dest: "{{ serverless_stage_dir }}/{{ item.dest }}"
        mode: 0755
      with_items:
        - { src: 'handler.py.j2', dest: '{{ serverless_handler_dest }}' }
        - { src: 'serverless.yml.j2', dest: 'serverless.yml' }

    - debug:
        msg:
          - ""
          - "Deploying cluster self-termination functionality:"
          - ""
          - "ClusterName      = {{ cluster_name }}"
          - "AvailabilityZone = {{ az }}"
          - "StartingTime     = {{ start_lambda_timer.stdout }}"
          - ""
          - "This operation typically completes within 5 minutes..."
          - ""

    - name: Deploy the Lambda function to terminate the cluster stack when cluster_lifetime has exceeded
      serverless:
        service_path: "{{ serverless_stage_dir }}"
        region: "{{ region }}"
        stage: "{{ prod_level }}"
        state: present
        verbose: true
      ignore_errors: true

    - name: Stop lambda_timer
      command: date +%Y-%m-%d\ \@\ %H:%M:%S
      register: stop_lambda_timer

    - name: Get the IP address of the master instance
      shell: pcluster status --region {{ region }} {{ cluster_name }} | grep MasterPublicIP | awk '{print $2}'
      register: MasterPublicIP

    - name: Accept the SSH fingerprint of the master instance
      shell: ssh-keyscan -H {{ MasterPublicIP.stdout }} >> {{ ssh_known_hosts }}

    - name: Create performance source tree and staging directories on the master instance
      command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} mkdir -p {{ item }}
      with_items:
        - "{{ master_performance_dir_dest }}"
        - "{{ stage_dir }}"

    - name: Transfer the local staging directory to the master instance
      command: scp -i {{ ssh_keypair }} -r {{ stage_dir }} {{ ec2_user }}@{{ MasterPublicIP.stdout }}:{{ stage_dir }}

    - name: Transfer the standard SGE submission script to the master instance
      command: scp -i {{ ssh_keypair }} {{ local_workingdir }}/qsub_default_submission_script.sh {{ ec2_user }}@{{ MasterPublicIP.stdout }}:{{ ec2_user_home }}
      when: 'scheduler == "sge"'

    - name: Transfer the standard Slurm submission script to the master instance
      command: scp -i {{ ssh_keypair }} {{ local_workingdir }}/sbatch_default_submission_script.sh {{ ec2_user }}@{{ MasterPublicIP.stdout }}:{{ ec2_user_home }}
      when: 'scheduler == "slurm"'

    - block:
      - name: Copy the performance source tree to its final destination directory on the master instance
        shell: scp -i {{ ssh_keypair }} {{ performance_stage_dir }}/* {{ ec2_user }}@{{ MasterPublicIP.stdout }}:{{ master_performance_dir_dest }} 
      - name: Symlink the custom shell scripts on the master instance
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} cd {{ master_performance_dir_dest }}; ln -s {{ item }}.{{ cluster_name }}.sh {{ item }}.sh
        with_items:
          - bang
          - combine_csv_summary_files_for_plotting
          - perf-standalone-test
        ignore_errors: true
      - name: Symlink the custom SGE shell scripts on the master instance
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} cd {{ master_performance_dir_dest }}; ln -s {{ item }}.{{ cluster_name }}.sh {{ item }}.sh
        with_items:
          - combine_sge_data_files_for_plotting
          - create_sge_task_array_csv_files
          - perf-qsub
        ignore_errors: true
        when: 'scheduler == "sge"'
      - name: Symlink the custom Slurm shell scripts on the master instance
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} cd {{ master_performance_dir_dest }}; ln -s {{ item }}.{{ cluster_name }}.sh {{ item }}.sh
        with_items:
          - perf-sbatch
        ignore_errors: true
        when: 'scheduler == "slurm"'
      - name: Symlink the custom Python scripts on the master instance
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} cd {{ master_performance_dir_dest }}; ln -s {{ item }}.{{ cluster_name }}.py {{ item }}.py
        with_items:
          - fibonacci_hashtest
          - hashtest
          - print_fibonacci
        ignore_errors: true
      when:
        - enable_hpc_performance_tests == "true"
        - scheduler != 'awsbatch'

    - block:
      - name: Create an EBS shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} sudo mkdir -p {{ ebs_hpc_performance_dir }}
      - name: Set ownership for the EBS shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} sudo chown -R {{ ec2_user }}:{{ ec2_user }} {{ ebs_hpc_performance_dir }}
      - name: Copy the final performance source tree from the master instance to the EBS shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} cp -a {{ master_performance_dir_dest }}/* {{ ebs_hpc_performance_dir }}
      when:
        - enable_hpc_performance_tests == "true"
        - scheduler != 'awsbatch'

    - block:
      - name: Create an EFS shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} sudo mkdir -p {{ efs_hpc_performance_dir }}
      - name: Set ownership for the EFS shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} sudo chown -R {{ ec2_user }}:{{ ec2_user }} {{ efs_hpc_performance_dir }}
      - name: Copy the final performance source tree from the master instance to the EFS shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} cp -a {{ master_performance_dir_dest }}/* {{ efs_hpc_performance_dir }}
      when:
        - enable_hpc_performance_tests == "true"
        - enable_efs == "true"
        - scheduler != 'awsbatch'

    - block:
      - name: Create an external NFS shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} sudo mkdir -p {{ external_nfs_hpc_performance_dir }}
      - name: Set ownership for the external NFS shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} sudo chown -R {{ ec2_user }}:{{ ec2_user }} {{ external_nfs_hpc_performance_dir }}
      - name: Copy the final performance source tree from the master instance to the external NFS shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} cp -a {{ master_performance_dir_dest }}/* {{ external_nfs_hpc_performance_dir }}
      when:
        - enable_hpc_performance_tests == "true"
        - enable_external_nfs == "true"
        - scheduler != 'awsbatch'

    - block:
      - name: Create an FSxL shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} sudo mkdir -p {{ fsx_hpc_performance_dir }}
      - name: Set ownership for the FSxL shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} sudo chown -R {{ ec2_user }}:{{ ec2_user }} {{ fsx_hpc_performance_dir }}
      - name: Copy the final performance source tree from the master instance to the external NFS shared storage HPC performance test directory
        command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} cp -a {{ master_performance_dir_dest }}/* {{ fsx_hpc_performance_dir }}
      when:
        - enable_hpc_performance_tests == "true"
        - enable_fsx == "true"
        - scheduler != 'awsbatch'

    - name: Remove the staging directory on the master instance
      command: ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }} rm -rf {{ stage_dir }}
      when:
        - enable_hpc_performance_tests == "true"
        - scheduler != 'awsbatch'

    - name: Copy the custom scripts from the local staging directory to the cluster_data directory
      shell: cp -a {{ stage_dir }}/* {{ cluster_data_dir }}

    - name: Copy the cluster_data directory to s3_bucketname
      s3_sync:
        bucket: "{{ s3_bucketname }}"
        file_root: "{{ cluster_data_dir }}"
        key_prefix: "{{ s3_cluster_data_dir }}"
        region: "{{ region }}" 

    - block:
      - name: Remove the local staging directory
        file:
          path: "{{ item }}"
          state: absent
        with_items:
          - "{{ stage_dir }}"
      - name: Remove all custom qsub performance templates from the src tree
        file:
          path: "{{ performance_template_dir }}/qsub-*.j2"
          state: absent
        when: 'scheduler == "sge"'
      - name: Remove all custom sbatch performance templates from the src tree
        file:
          path: "{{ performance_template_dir }}/sbatch-*.j2"
          state: absent
        when: 'scheduler == "slurm"'
      when:
        - enable_hpc_performance_tests == "true"
        - scheduler != 'awsbatch'

    - name: Stop the overall stack timer
      command: date +%Y-%m-%d\ \@\ %H:%M:%S
      register: stop_overall_timer

    - name: Parse cluster_end_time from cluster_serial_number_file
      shell: cat {{ cluster_serial_number_file }} | grep cluster_end_time
      register: cluster_end_time

    - name: Template the cluster build summary report
      template:
        src: "{{ sns_build_summary_report_src }}"
        dest: "{{ sns_build_summary_report_dest }}"
        mode: 0755

    - name: Publish the cluster build summary report to the SNS endpoint
      sns:
        msg: "{{ lookup('file', '{{ sns_build_summary_report_dest }}') }}"
        subject: "Cluster Deployment Update: {{ cluster_name }}"
        topic: sns_alerts_{{ cluster_name }}
        region: "{{ region }}"
      delegate_to: localhost

    - debug:
         msg:
          - "=================================================================="
          - "                   Cluster Build Summary Report"
          - "=================================================================="
          - ""
          - "Launched the environment: {{ start_overall_timer.stdout }}"
          - "Initiated stack creation: {{ start_stack_creation_timer.stdout }}"
          - "Completed stack creation: {{ stop_stack_creation_timer.stdout }}"
          - "Finished the environment: {{ stop_overall_timer.stdout }}"
          - "Lifetime (days:hr:min):   {{ cluster_lifetime }}"
          - ""
          - "Cluster Stack Name:    {{ cluster_name }}"
          - "SerialDateStamp:       {{ cluster_serial_datestamp }}"
          - "AWS Availability Zone: {{ az }}"
          - "Master Instance Type:  {{ master_instance_type }}"
          - "Compute Instance Type: {{ compute_instance_type }}"
          - "Operating System:      {{ base_os }}"
          - "HPC Scheduler Type:    {{ scheduler }}" 
          - ""
          - "EFA Enabled:  {{ enable_efa | bool | upper }}"
          - "EFS Enabled:  {{ enable_efs | bool | upper }}"
          - "FSxL Enabled: {{ enable_fsx | bool | upper }}"
          - "External NFS: {{ enable_external_nfs | bool | upper }}"
          - ""
          - "Choose an option to access the cluster's master instance:"
          - ""
          - "(1) ParallelClusterMaker access script:"
          - "    ./access_cluster.py -N {{ cluster_name }}"
          - ""
          - "(2) ParallelCluster ssh alias:"
          - "    pcluster ssh {{ cluster_name }} -i {{ ssh_keypair }}"
          - ""
          - "(3) Vanilla ssh with the cluster's private SSH key:"
          - "    ssh -i {{ ssh_keypair }} {{ ec2_user }}@{{ MasterPublicIP.stdout }}"
          - ""
          - "To destroy this cluster, choose one of the following options:"
          - ""
          - "(1) ParallelClusterMaker terminate stack wrapper script:"
          - "    .{{ cluster_data_dir }}/kill_pcluster.{{ cluster_name }}.sh"
          - ""
          - "(2) Manually with the kill-pcluster Python script:"
          - "    ./kill-pcluster.py -N {{ cluster_birth_name }} -O {{ cluster_owner }} -A {{ az }}"
          - ""
          - "(3) Wait for cluster_lifetime to expire."
          - ""

    - debug:
         msg:
          - "Visit this link to view cluster statistics:"
          - "    http://{{ MasterPublicIP.stdout }}/ganglia"
          - ""
      when: 'enable_ganglia == "true"'

    - debug:
         msg:
          - ""
          - "S3-Lustre import path: s3://{{ fsx_s3_import_bucket }}/{{ fsx_s3_import_path }}"
          - "Lustre-S3 export path: s3://{{ fsx_s3_export_bucket }}/{{ fsx_s3_export_path }}"
          - ""
          - "Import S3 from Lustre:    /usr/local/bin/import-s3-to-lustre.sh"
          - "Export Lustre to S3:      /usr/local/bin/export-lustre-to-s3.sh"
          - "Check export job status:  /usr/local/bin/check-lustre-export-progress.sh"
          - ""
      when: 'enable_fsx_hydration == "true"'
